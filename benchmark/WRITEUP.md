# sqlitesearch Benchmark Results

## Recommendation

sqlitesearch is designed for small, local projects — no servers, no dependencies beyond numpy, just a single SQLite file.

| Use case | Scale | Mode | Verdict |
|----------|------:|------|---------|
| Text search | up to 125K docs | — | Excellent — 2.4x faster than minsearch |
| Text search | 291K docs | — | Works but slower (11 QPS) |
| Vector search | up to 100K | HNSW | Best — 0.97 recall, 5ms, 209 QPS |
| Vector search | up to 100K | IVF | Good — 0.86 recall, 29ms, 35 QPS |
| Vector search | up to 100K | LSH | OK — 0.89 recall with n_probe=2, 181ms |
| Vector search | 1M | IVF | Usable — 0.92 recall, 219ms, 6 min build |
| Vector search | 1M | HNSW | Fast search (2ms) but slow build (21+ min) |
| Vector search | 1M | LSH | Too slow — max 0.82 recall at 4s latency |

---

## Text Search Benchmark

### Dataset

Simple English Wikipedia: 291,737 articles (~1 GB JSONL).

### Results

| Docs | Indexing | DB size | Search avg | QPS |
|------|----------|---------|-----------|-----|
| 1K | 0.57s | 51 MB | 1.03 ms | 970 |
| 10K | 6.54s | 346 MB | 1.66 ms | 604 |
| 50K | 23.62s | 1,050 MB | 3.45 ms | 290 |
| 125K | 60.41s | 2,549 MB | 5.59 ms | 179 |
| 291K | 153.51s | 5,559 MB | 90.62 ms | 11 |

### Comparison: sqlitesearch vs minsearch (125K docs)

| Metric | minsearch Regular | minsearch Appendable | sqlitesearch |
|--------|-------------------|----------------------|-----------------|
| Indexing | 57.76s | 79.08s | 60.41s |
| Search avg | 1,043.73 ms | 13.66 ms | 5.59 ms |
| QPS | 1.0 | 73 | 179 |
| 291K docs | OOM | OOM | Works (11 QPS) |
| Persistence | No | No | Yes (SQLite) |
| RAM usage | ~6 GB | ~6 GB | Minimal |

### Text search optimizations applied

1. SQLite WAL mode + cache tuning — `journal_mode=WAL`, `synchronous=NORMAL`, 64 MB cache
2. Batch inserts — `executemany()` for both docs and FTS5 tables
3. Stopwords removal — filter 174 common English words before FTS5 query
4. Subquery optimization — rank inside FTS5 subquery with LIMIT, then JOIN only top results

Impact at 291K docs: baseline 11,611ms/query → optimized 91ms/query (128x faster).

---

## Vector Search Benchmark

### Dataset

Cohere Wikipedia-22-12 Medium (1M) — 768-dimensional embeddings of English Wikipedia articles, generated by the [Cohere Embed v2](https://cohere.com/embed) model. This is the standard dataset used by the [VDBBench leaderboard](https://zilliz.com/vdbbench-leaderboard) for vector database benchmarking.

- Source: Hosted by [Zilliz](https://zilliz.com/) on public S3 (anonymous access, `us-west-2`):
  - `s3://assets.zilliz.com/benchmark/cohere_medium_1m/shuffle_train.parquet` — 1M train vectors (~3 GB)
  - `s3://assets.zilliz.com/benchmark/cohere_medium_1m/test.parquet` — 1K query vectors
  - `s3://assets.zilliz.com/benchmark/cohere_medium_1m/neighbors.parquet` — pre-computed ground truth (top-100 neighbors)
- Dimensions: 768 (float32)
- Similarity metric: Cosine
- Download: The benchmark script expects files at `/tmp/vectordb_bench/dataset/cohere_medium_1m/`. Use the download function in `bench_vdbbench.py` or download manually with [s3fs](https://pypi.org/project/s3fs/) / AWS CLI. This is the same dataset used by the [VDBBench leaderboard](https://zilliz.com/vdbbench-leaderboard)

### How multi-probe LSH works

Standard LSH hashes each vector into a bucket per table using random projections. At query time, it looks up the query's exact bucket in each table. The problem: with 16 hash bits, there are 65,536 possible buckets per table, and two similar vectors (cosine similarity 0.9) only have ~8.7% chance of landing in the same bucket. Even with 8 tables, there's a ~50% chance of missing a true nearest neighbor entirely — and much worse for the top-100.

Multi-probe LSH fixes this by also checking neighboring buckets. Instead of looking up just the query's exact hash, it flips 1 or 2 bits and checks those buckets too. With `n_probe=2` (flip up to 2 bits), each table checks 1 + 16 + 120 = 137 buckets instead of 1. This raises the probability of finding a neighbor with cosine similarity 0.9 from ~50% to ~99%.

The tradeoff is more candidates to rerank. To keep reranking fast, vectors are cached in a numpy array in memory (loaded once from SQLite on first search), so the cosine similarity computation is a single matrix-vector multiply — essentially free compared to the SQL lookup.

### Results (seed=42, default: 8 tables, 16 hash bits, n_probe=0)

| N vectors | Insert | vec/s | Recall@10 | Recall@100 | Avg lat | P99 lat | QPS | DB size |
|----------:|-------:|------:|----------:|-----------:|--------:|--------:|----:|--------:|
| 1,000 | 0.13s | 7,944 | 0.29 | 0.13 | 0.5ms | 0.9ms | 2,152 | 5 MB |
| 10,000 | 0.86s | 11,695 | 0.31 | 0.19 | 6.2ms | 19.5ms | 162 | 47 MB |
| 100,000 | 8.90s | 11,233 | 0.29 | 0.20 | 15.6ms | — | 64 | 466 MB |
| 1,000,000 | 123s | 8,221 | 0.31 | 0.23 | 128ms | — | 8 | 4,666 MB |

With the default n_probe=0, recall is low but latency is fast. Setting n_probe=2 trades latency for much higher recall (see tuning section below).

### Recall tuning at 100K

Recall depends on three LSH parameters: `n_tables`, `hash_size`, and `n_probe`. Tested on 100K Cohere vectors (768d) with brute-force ground truth:

| n_tables | hash_size | n_probe | Recall@10 | Recall@100 | Latency | QPS |
|---------:|----------:|--------:|----------:|-----------:|--------:|----:|
| 8 (default) | 16 | 0 | 0.29 | 0.20 | 16ms | 64 |
| 8 | 16 | 1 | 0.83 | 0.70 | 88ms | 11 |
| 8 | 16 | 2 | 0.95 | 0.89 | 181ms | 6 |
| 16 | 16 | 1 | 0.95 | 0.87 | 151ms | 7 |
| 16 | 16 | 2 | 0.99 | 0.95 | 258ms | 4 |
| 32 | 16 | 1 | 0.99 | 0.96 | 209ms | 5 |

```python
# Default: fast, low recall
index = VectorSearchIndex(db_path="vectors.db")

# Better recall (0.89 at 100K)
index = VectorSearchIndex(n_probe=2, db_path="vectors.db")

# Best recall (0.95 at 100K, slower inserts, larger DB)
index = VectorSearchIndex(n_tables=16, n_probe=2, db_path="vectors.db")
```

### Recall tuning at 1M (n_probe=0 only)

At 1M, we tested whether more tables and fewer hash bits can compensate for no multi-probe. All configs use n_probe=0:

| n_tables | hash_size | Recall@10 | Recall@100 | Latency | QPS | Insert | DB size |
|---------:|----------:|----------:|-----------:|--------:|----:|-------:|--------:|
| 8 | 16 | 0.31 | 0.23 | 128ms | 8 | 123s | 4.7 GB |
| 8 | 10 | 0.38 | 0.28 | 418ms | 2 | 87s | 4.5 GB |
| 8 | 8 | 0.41 | 0.28 | 618ms | 2 | 82s | 4.5 GB |
| 16 | 8 | 0.57 | 0.42 | 1,089ms | 1 | 135s | 5.0 GB |
| 32 | 8 | 0.78 | 0.60 | 2,100ms | 0.5 | 258s | 6.1 GB |
| 32 | 6 | 0.78 | 0.62 | 3,197ms | 0.3 | 221s | 5.9 GB |
| 64 | 10 | 0.90 | 0.76 | 2,327ms | 0.4 | 808s | 8.7 GB |
| 64 | 8 | 0.95 | 0.81 | 3,993ms | 0.3 | 567s | 8.3 GB |
| 64 | 6 | 0.94 | 0.82 | 6,450ms | 0.2 | 438s | 7.9 GB |

Even with 64 tables and 6 hash bits, recall@100 tops out at 0.82 with 6.5s latency. Without multi-probe, you need so many tables that every query scans a large fraction of the dataset — at which point you've lost the benefit of LSH.

### VDBBench leaderboard comparison (Cohere-1M)

```
Database                            QPS  P99(ms)   Recall
----------------------------------------------------------
ZillizCloud-8cu-perf              9,704      2.5   0.9170
Milvus-16c64g-sq8                 3,465      2.2   0.9530
OpenSearch-16c128g-fm             3,055      7.2   0.9070
ElasticCloud-8c60g-fm             1,925     11.3   0.8960
QdrantCloud-16c64g                1,242      6.4   0.9470
Pinecone-p2.x8                    1,147     13.7   0.9260
----------------------------------------------------------
sqlitesearch HNSW-fast [1M]         499      2.9   0.7023
sqlitesearch IVF/16p [1M]           4.6    396.0   0.8602
sqlitesearch HNSW [100K]            209      5.5   0.9731
sqlitesearch IVF/16p [100K]          35     56.2   0.8602
sqlitesearch LSH/p2 [100K]            6    227.2   0.8897
```

Note: Leaderboard = multi-process on cloud hardware (8-16 cores, 32-128GB RAM). sqlitesearch = serial single-process in pure Python.

### Vector search optimizations applied

1. Multi-probe LSH — probe neighboring hash buckets (1-bit and 2-bit flips) to increase recall. `n_probe=2` probes 137 buckets per table instead of 1
2. In-memory vector cache — vectors are cached in a numpy array after insert/load, eliminating SQLite I/O during reranking
3. Per-table candidate queries — separate SQL query per hash table merged in Python (faster than one large OR query with GROUP BY)
4. Vectorized batch hashing — single matmul for all vectors x all tables during insert
5. Vectorized query hashing — single matmul for all tables during search
6. Raw bytes instead of pickle — `tobytes()`/`frombuffer()` for vector storage
7. Vectorized cosine reranking — matrix multiply + `argpartition` for top-K
8. Chunked IN-queries — avoids SQLite variable limit for large candidate sets
9. Multi-probe candidate cap — limits reranking to top 50K candidates by table-hit count

### Why LSH doesn't scale to 1M

At 1M with 768 dimensions, LSH with random projections reaches its limits. The top-100 nearest neighbors often have moderate cosine similarity (0.6-0.8), where the probability of landing in the same or nearby hash bucket is low even with multi-probe. Without multi-probe, getting competitive recall requires 64 tables and small hash sizes, which makes every query scan a large fraction of the dataset (6+ seconds per query, 8+ GB database).

---

## IVF (Inverted File Index)

IVF clusters vectors using k-means, then at query time searches only the nearest clusters. Number of clusters auto-scales as `min(sqrt(n), 256)`. The `n_probe_clusters` parameter controls how many clusters are searched (tradeoff: more probes = higher recall, slower search).

### IVF results at 100K (Cohere 768d)

| Config | R@10 | R@100 | Avg latency | P99 latency | QPS | Build | DB size |
|--------|------|-------|-------------|-------------|-----|-------|---------|
| IVF 4 probes | 0.755 | 0.603 | 9.0ms | 32.6ms | 111 | 36s | 399 MB |
| IVF 8 probes | 0.866 | 0.746 | 14.8ms | 32.0ms | 68 | 36s | 399 MB |
| IVF 16 probes | 0.922 | 0.860 | 28.6ms | 56.2ms | 35 | 39s | 399 MB |

### IVF results at 1M (Cohere 768d)

| Config | R@10 | R@100 | Avg latency | P99 latency | QPS | Build | DB size |
|--------|------|-------|-------------|-------------|-----|-------|---------|
| IVF 4 probes | 0.799 | 0.619 | 124ms | 345ms | 8.1 | 368s | 3,987 MB |
| IVF 8 probes | 0.852 | 0.746 | 124ms | 205ms | 8.1 | 367s | 3,987 MB |
| IVF 16 probes | 0.922 | 0.860 | 219ms | 396ms | 4.6 | 373s | 3,987 MB |

IVF is the best mode for 1M: 6-minute build, 0.86-0.92 recall, and reasonable latency. The k-means build is fast because it's pure numpy vectorized.

```python
# IVF with 16 probe clusters (best recall)
index = VectorSearchIndex(mode="ivf", n_probe_clusters=16, db_path="vectors.db")

# IVF with 8 probes (balanced)
index = VectorSearchIndex(mode="ivf", n_probe_clusters=8, db_path="vectors.db")
```

---

## HNSW (Hierarchical Navigable Small World)

HNSW builds a multi-layer proximity graph for fast approximate nearest neighbor search. The graph is stored in SQLite but loaded into memory on first search. Parameters:
- `m`: max connections per node (higher = better recall, slower build, larger DB)
- `ef_construction`: beam width during graph building (higher = better graph quality, slower build)
- `ef_search`: beam width during search (higher = better recall, slower search)

### HNSW results at 100K (Cohere 768d)

| Config | R@10 | R@100 | Avg latency | P99 latency | QPS | Build | DB size |
|--------|------|-------|-------------|-------------|-----|-------|---------|
| m16/ef_c200/ef_s50 | 0.898 | 0.497 | 2.1ms | 2.6ms | 482 | 473s | 547 MB |
| m16/ef_c200/ef_s100 | 0.918 | 0.887 | 2.2ms | 2.8ms | 456 | 453s | 547 MB |
| m32/ef_c200/ef_s200 | 0.971 | 0.973 | 4.8ms | 5.5ms | 209 | 756s | 696 MB |

### HNSW results at 1M (Cohere 768d, fast-build: m=12, ef_construction=32)

| Config | R@10 | R@100 | Avg latency | P99 latency | QPS | Build | DB size |
|--------|------|-------|-------------|-------------|-----|-------|---------|
| ef_search=50 | 0.661 | 0.439 | 1.2ms | 2.1ms | 854 | 1294s | 5,196 MB |
| ef_search=100 | 0.723 | 0.702 | 2.0ms | 2.9ms | 499 | 1285s | 5,196 MB |

HNSW excels at search speed (1-5ms) but the pure-Python graph build is slow at scale. At 100K, the high-quality config (m32/ef200) achieves 0.97 recall with 5ms latency. At 1M, the fast-build config needs ~21 min to build and achieves 0.70 recall. The high-quality 1M build would take 1-2 hours.

```python
# HNSW high quality (best for ≤100K)
index = VectorSearchIndex(mode="hnsw", m=32, ef_construction=200, ef_search=200, db_path="vectors.db")

# HNSW fast build (for larger datasets)
index = VectorSearchIndex(mode="hnsw", m=12, ef_construction=32, ef_search=100, db_path="vectors.db")
```

---

## Mode comparison at 100K

| Mode | Config | R@10 | R@100 | Latency | QPS | Build | DB |
|------|--------|------|-------|---------|-----|-------|----|
| HNSW | m32/ef200 | **0.971** | **0.973** | **4.8ms** | **209** | 756s | 696 MB |
| HNSW | m16/ef100 | 0.918 | 0.887 | 2.2ms | 456 | 453s | 547 MB |
| IVF | 16 probes | 0.922 | 0.860 | 28.6ms | 35 | 39s | 399 MB |
| IVF | 8 probes | 0.866 | 0.746 | 14.8ms | 68 | 36s | 399 MB |
| LSH | n_probe=2 | 0.950 | 0.890 | 181ms | 6 | 9s | 466 MB |
| LSH | 32t/16b/p1 | 0.990 | 0.960 | 209ms | 5 | — | — |

At 100K, HNSW dominates: highest recall with lowest latency. IVF offers a good balance with fast builds. LSH has the fastest build but slowest search.

## Mode comparison at 1M

| Mode | Config | R@10 | R@100 | Latency | QPS | Build | DB |
|------|--------|------|-------|---------|-----|-------|----|
| IVF | 16 probes | **0.922** | **0.860** | 219ms | 4.6 | **373s** | 3,987 MB |
| IVF | 8 probes | 0.852 | 0.746 | 124ms | 8.1 | 367s | 3,987 MB |
| HNSW-fast | ef_s=100 | 0.723 | 0.702 | **2.0ms** | **499** | 1285s | 5,196 MB |
| HNSW-fast | ef_s=50 | 0.661 | 0.439 | 1.2ms | 854 | 1294s | 5,196 MB |
| LSH | 64t/8b | 0.950 | 0.810 | 3993ms | 0.3 | 567s | 8,300 MB |

At 1M, IVF is the recommended mode: best recall with reasonable latency and the fastest build. HNSW has the fastest search but lower recall with fast-build settings and much longer build times. LSH is impractical at this scale.

---

All 124 tests pass after all optimizations (102 original + 11 IVF + 11 HNSW).
