# sqlitesearch Benchmark Results

## Recommendation

sqlitesearch is designed for small, local projects — no servers, no dependencies beyond numpy, just a single SQLite file.

| Use case | Scale | Verdict |
|----------|------:|---------|
| Text search | up to 125K docs | Excellent — 2.4x faster than minsearch |
| Text search | 291K docs | Works but slower (11 QPS) |
| Vector search | up to 100K vectors | Good — 0.89 recall, ~180ms latency |
| Vector search | 1M+ vectors | Too slow — use a dedicated vector DB |

---

## Text Search Benchmark

### Dataset

Simple English Wikipedia: 291,737 articles (~1 GB JSONL).

### Results

| Docs | Indexing | DB size | Search avg | QPS |
|------|----------|---------|-----------|-----|
| 1K | 0.57s | 51 MB | 1.03 ms | 970 |
| 10K | 6.54s | 346 MB | 1.66 ms | 604 |
| 50K | 23.62s | 1,050 MB | 3.45 ms | 290 |
| 125K | 60.41s | 2,549 MB | 5.59 ms | 179 |
| 291K | 153.51s | 5,559 MB | 90.62 ms | 11 |

### Comparison: sqlitesearch vs minsearch (125K docs)

| Metric | minsearch Regular | minsearch Appendable | sqlitesearch |
|--------|-------------------|----------------------|-----------------|
| Indexing | 57.76s | 79.08s | 60.41s |
| Search avg | 1,043.73 ms | 13.66 ms | 5.59 ms |
| QPS | 1.0 | 73 | 179 |
| 291K docs | OOM | OOM | Works (11 QPS) |
| Persistence | No | No | Yes (SQLite) |
| RAM usage | ~6 GB | ~6 GB | Minimal |

### Text search optimizations applied

1. SQLite WAL mode + cache tuning — `journal_mode=WAL`, `synchronous=NORMAL`, 64 MB cache
2. Batch inserts — `executemany()` for both docs and FTS5 tables
3. Stopwords removal — filter 174 common English words before FTS5 query
4. Subquery optimization — rank inside FTS5 subquery with LIMIT, then JOIN only top results

Impact at 291K docs: baseline 11,611ms/query → optimized 91ms/query (128x faster).

---

## Vector Search Benchmark

### Dataset

Cohere Wikipedia-22-12 Medium (1M) — 768-dimensional embeddings of English Wikipedia articles, generated by the [Cohere Embed v2](https://cohere.com/embed) model. This is the standard dataset used by the [VDBBench leaderboard](https://zilliz.com/vdbbench-leaderboard) for vector database benchmarking.

- Source: Hosted by [Zilliz](https://zilliz.com/) on public S3 (anonymous access, `us-west-2`):
  - `s3://assets.zilliz.com/benchmark/cohere_medium_1m/shuffle_train.parquet` — 1M train vectors (~3 GB)
  - `s3://assets.zilliz.com/benchmark/cohere_medium_1m/test.parquet` — 1K query vectors
  - `s3://assets.zilliz.com/benchmark/cohere_medium_1m/neighbors.parquet` — pre-computed ground truth (top-100 neighbors)
- Dimensions: 768 (float32)
- Similarity metric: Cosine
- Download: The benchmark script expects files at `/tmp/vectordb_bench/dataset/cohere_medium_1m/`. Use the download function in `bench_vdbbench.py` or download manually with [s3fs](https://pypi.org/project/s3fs/) / AWS CLI. This is the same dataset used by the [VDBBench leaderboard](https://zilliz.com/vdbbench-leaderboard)

### How multi-probe LSH works

Standard LSH hashes each vector into a bucket per table using random projections. At query time, it looks up the query's exact bucket in each table. The problem: with 16 hash bits, there are 65,536 possible buckets per table, and two similar vectors (cosine similarity 0.9) only have ~8.7% chance of landing in the same bucket. Even with 8 tables, there's a ~50% chance of missing a true nearest neighbor entirely — and much worse for the top-100.

Multi-probe LSH fixes this by also checking neighboring buckets. Instead of looking up just the query's exact hash, it flips 1 or 2 bits and checks those buckets too. With `n_probe=2` (flip up to 2 bits), each table checks 1 + 16 + 120 = 137 buckets instead of 1. This raises the probability of finding a neighbor with cosine similarity 0.9 from ~50% to ~99%.

The tradeoff is more candidates to rerank. To keep reranking fast, vectors are cached in a numpy array in memory (loaded once from SQLite on first search), so the cosine similarity computation is a single matrix-vector multiply — essentially free compared to the SQL lookup.

### Results (seed=42, 8 tables, 16 hash bits, n_probe=2)

| N vectors | Insert | vec/s | Recall@100 | Avg lat | P99 lat | QPS | DB size |
|----------:|-------:|------:|-----------:|--------:|--------:|----:|--------:|
| 1,000 | 0.13s | 7,944 | 0.6467 | 3.0ms | 4.5ms | 333 | 5 MB |
| 10,000 | 0.86s | 11,695 | 0.9708 | 25.7ms | 60.9ms | 39 | 47 MB |
| 100,000 | 8.90s | 11,233 | 0.8897 | 180.7ms | 227.2ms | 6 | 466 MB |
| 1,000,000 | 122s | 8,221 | 0.5226 | 1,116ms | 1,687ms | 1 | 4,666 MB |

### Recall tuning

Recall depends on three LSH parameters: `n_tables`, `hash_size`, and `n_probe`. Higher values improve recall at the cost of speed. Tested on 100K Cohere vectors (768d) with brute-force ground truth:

| n_tables | hash_size | n_probe | Recall@10 | Recall@100 | Latency | QPS |
|---------:|----------:|--------:|----------:|-----------:|--------:|----:|
| 8 | 16 | 0 | 0.29 | 0.20 | 16ms | 64 |
| 8 (default) | 16 | 2 | 0.95 | 0.89 | 181ms | 6 |
| 16 | 16 | 1 | 0.95 | 0.87 | 151ms | 7 |
| 16 | 16 | 2 | 0.99 | 0.95 | 258ms | 4 |
| 32 | 16 | 1 | 0.99 | 0.96 | 209ms | 5 |

For higher recall, increase `n_tables`:

```python
# Default: 0.89 recall at 100K
index = VectorSearchIndex(db_path="vectors.db")

# Higher recall: 0.95 recall at 100K (slower inserts, larger DB)
index = VectorSearchIndex(n_tables=16, n_probe=2, db_path="vectors.db")
```

### VDBBench leaderboard comparison (Cohere-1M)

```
Database                            QPS  P99(ms)   Recall
----------------------------------------------------------
ZillizCloud-8cu-perf              9,704      2.5   0.9170
Milvus-16c64g-sq8                 3,465      2.2   0.9530
OpenSearch-16c128g-fm             3,055      7.2   0.9070
ElasticCloud-8c60g-fm             1,925     11.3   0.8960
QdrantCloud-16c64g                1,242      6.4   0.9470
Pinecone-p2.x8                    1,147     13.7   0.9260
----------------------------------------------------------
sqlitesearch [100K]                   6    227.2   0.8897
sqlitesearch [1M]                     1  1,687.4   0.5226
```

Note: Leaderboard = multi-process on cloud hardware (8-16 cores, 32-128GB RAM). sqlitesearch = serial single-process.

### Vector search optimizations applied

1. Multi-probe LSH — probe neighboring hash buckets (1-bit and 2-bit flips) to dramatically increase recall. Default `n_probe=2` probes 137 buckets per table instead of 1
2. In-memory vector cache — vectors are cached in a numpy array after insert/load, eliminating SQLite I/O during reranking
3. Per-table candidate queries — separate SQL query per hash table merged in Python (faster than one large OR query with GROUP BY)
4. Vectorized batch hashing — single matmul for all vectors x all tables during insert
5. Vectorized query hashing — single matmul for all tables during search
6. Raw bytes instead of pickle — `tobytes()`/`frombuffer()` for vector storage
7. Vectorized cosine reranking — matrix multiply + `argpartition` for top-K
8. Chunked IN-queries — avoids SQLite variable limit for large candidate sets
9. Multi-probe candidate cap — limits reranking to top 50K candidates by table-hit count

### Recall improvement timeline

| Version | Recall@100 (100K) | Avg latency (100K) |
|---------|------------------:|-------------------:|
| Before multi-probe (n_probe=0) | 0.20 | 56ms |
| Multi-probe n_probe=1 | 0.70 | 107ms |
| Multi-probe n_probe=2 + cache | 0.89 | 181ms |

### Why 1M doesn't scale

At 1M with 768 dimensions, LSH with random projections reaches its limits. The top-100 nearest neighbors often have moderate cosine similarity (0.6-0.8), where the probability of landing in the same or nearby hash bucket is low. Even with multi-probe (137 buckets per table), the candidate pool at 1M only covers ~1% of the dataset — not enough for high recall. Purpose-built vector databases use HNSW or IVF indexes with in-memory graph traversal, which scales to millions of vectors.

---

All 102 existing tests pass after all optimizations.
